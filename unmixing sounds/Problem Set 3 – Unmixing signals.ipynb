{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unmixing signals with ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unmixing sound signals is an example of cocktail party problem you are going to use for getting hands-on experience with ICA. You have 5 mixed sound sources in **mixed** folder (go check them out). Your goal is to unmix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from WAV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from WAV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53442, 5)\n",
      "[[ 343 -546 -327 -275  612]\n",
      " [ 627 -840 -579 -124  890]\n",
      " [ 589 -725 -491 -115  989]\n",
      " [ 712 -887 -571  -24 1111]\n",
      " [ 589 -725 -491 -115  989]\n",
      " [ 268 -462 -146 -236  678]\n",
      " [ 107 -330   27 -296  522]\n",
      " [-214  -67  372 -416  211]\n",
      " [-214  -67  372 -416  211]\n",
      " [ 159 -206  -26 -233  445]]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for i in range(1,6):\n",
    "    sample_rate, wav_data = scipy.io.wavfile.read('mixed/mix'+str(i)+'.wav')\n",
    "    dataset.append(wav_data)\n",
    "\n",
    "dataset = np.array(dataset).T\n",
    "print(dataset.shape)\n",
    "print(dataset[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01046796 -0.01666328 -0.00997965 -0.00839268  0.01867752]\n",
      " [ 0.0191353  -0.02563581 -0.0176704  -0.00378433  0.02716175]\n",
      " [ 0.01797558 -0.02212614 -0.01498474 -0.00350966  0.03018311]\n",
      " [ 0.0217294  -0.02707019 -0.01742625 -0.00073245  0.03390641]\n",
      " [ 0.01797558 -0.02212614 -0.01498474 -0.00350966  0.03018311]\n",
      " [ 0.00817904 -0.01409969 -0.00445575 -0.00720244  0.02069176]\n",
      " [ 0.00326551 -0.01007121  0.00082401 -0.00903357  0.01593082]\n",
      " [-0.00653103 -0.00204476  0.011353   -0.01269583  0.00643947]\n",
      " [-0.00653103 -0.00204476  0.011353   -0.01269583  0.00643947]\n",
      " [ 0.00485249 -0.00628688 -0.00079349 -0.00711089  0.01358087]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53442, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxs = np.max(np.abs(dataset), axis=0).astype(np.int64)\n",
    "data_normalized = 0.99 * dataset / maxs;\n",
    "print(data_normalized[:10,:])\n",
    "data_normalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing unmixing matrix $ W $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.identity(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement learning unmixing matrix $ W $ with ICA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1    Loss: 99.490\n",
      "Iteration:  2    Loss: 15.333\n",
      "Iteration:  3    Loss: 8.542\n",
      "Iteration:  4    Loss: 5.434\n",
      "Iteration:  5    Loss: 4.147\n",
      "Iteration:  6    Loss: 3.584\n",
      "Iteration:  7    Loss: 3.263\n",
      "Iteration:  8    Loss: 3.013\n",
      "Iteration:  9    Loss: 2.774\n",
      "Iteration: 10    Loss: 2.532\n",
      "Iteration: 11    Loss: 2.294\n",
      "Iteration: 12    Loss: 2.074\n",
      "Iteration: 13    Loss: 1.886\n",
      "Iteration: 14    Loss: 1.738\n",
      "Iteration: 15    Loss: 1.633\n",
      "Iteration: 16    Loss: 1.573\n",
      "Iteration: 17    Loss: 1.556\n",
      "Iteration: 18    Loss: 1.584\n",
      "Iteration: 19    Loss: 1.659\n",
      "Iteration: 20    Loss: 1.786\n",
      "Iteration: 21    Loss: 1.978\n",
      "Iteration: 22    Loss: 2.262\n",
      "Iteration: 23    Loss: 2.684\n",
      "Iteration: 24    Loss: 3.340\n",
      "Iteration: 25    Loss: 4.402\n",
      "Iteration: 26    Loss: 5.913\n",
      "Iteration: 27    Loss: 6.633\n",
      "Iteration: 28    Loss: 7.094\n",
      "Iteration: 29    Loss: 7.605\n",
      "Iteration: 30    Loss: 7.464\n",
      "Iteration: 31    Loss: 6.872\n",
      "Iteration: 32    Loss: 5.970\n",
      "Iteration: 33    Loss: 4.943\n",
      "Iteration: 34    Loss: 3.981\n",
      "Iteration: 35    Loss: 3.207\n",
      "Iteration: 36    Loss: 2.652\n",
      "Iteration: 37    Loss: 2.277\n",
      "Iteration: 38    Loss: 2.031\n",
      "Iteration: 39    Loss: 1.874\n",
      "Iteration: 40    Loss: 1.785\n",
      "Iteration: 41    Loss: 1.754\n",
      "Iteration: 42    Loss: 1.779\n",
      "Iteration: 43    Loss: 1.862\n",
      "Iteration: 44    Loss: 2.011\n",
      "Iteration: 45    Loss: 2.248\n",
      "Iteration: 46    Loss: 2.612\n",
      "Iteration: 47    Loss: 3.180\n",
      "Iteration: 48    Loss: 4.107\n",
      "Iteration: 49    Loss: 5.573\n",
      "Iteration: 50    Loss: 6.703\n",
      "Iteration: 51    Loss: 6.873\n",
      "Iteration: 52    Loss: 7.495\n",
      "Iteration: 53    Loss: 7.456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-da671e549152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration: {0:2d}    Loss: {1:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mlearn_optimal_W\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# ====================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-da671e549152>\u001b[0m in \u001b[0;36mlearn_optimal_W\u001b[0;34m(W, alpha, epsilon)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_normalized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mleft_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mW\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleft_matrix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# calc loss as norm of w - previous_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/linalg/linalg.pyc\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============== TODO: Your code here ===============\n",
    "# Implement learning unmixing matrix W with ICA. Do not forget to account for the dimensionality.\n",
    "alpha = 0.1\n",
    "\n",
    "def learn_optimal_W(W, alpha, epsilon=0.01):\n",
    "    convergence = False\n",
    "    iteration = 0\n",
    "    loss_history = []\n",
    "\n",
    "    while not convergence:\n",
    "        previous_W = deepcopy(W)\n",
    "\n",
    "        # gradient ascent\n",
    "        for item in data_normalized:\n",
    "            left_matrix = np.dot((1 - 2 * sigmoid(np.dot(W, item.T)[:,np.newaxis])), item[np.newaxis, :])\n",
    "            W += alpha * (left_matrix + np.linalg.inv(W.T))\n",
    "\n",
    "        # calc loss as norm of w - previous_W\n",
    "        loss = np.linalg.norm(W - previous_W)    \n",
    "        if loss_history and abs(loss - loss_history[-1]) < epsilon:\n",
    "            convergence = True\n",
    "\n",
    "        iteration += 1\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        print(\"Iteration: {0:2d}    Loss: {1:.3f}\".format(iteration, loss))\n",
    "\n",
    "learn_optimal_W(W, alpha)\n",
    "# ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this learning rate (alpha=0.1) is too large. Let's try smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1    Loss: 82.462\n",
      "Iteration:  2    Loss: 14.983\n",
      "Iteration:  3    Loss: 8.963\n",
      "Iteration:  4    Loss: 5.999\n",
      "Iteration:  5    Loss: 4.379\n",
      "Iteration:  6    Loss: 3.616\n",
      "Iteration:  7    Loss: 3.214\n",
      "Iteration:  8    Loss: 2.877\n",
      "Iteration:  9    Loss: 2.525\n",
      "Iteration: 10    Loss: 2.181\n",
      "Iteration: 11    Loss: 1.873\n",
      "Iteration: 12    Loss: 1.609\n",
      "Iteration: 13    Loss: 1.387\n",
      "Iteration: 14    Loss: 1.201\n",
      "Iteration: 15    Loss: 1.047\n",
      "Iteration: 16    Loss: 0.919\n",
      "Iteration: 17    Loss: 0.812\n",
      "Iteration: 18    Loss: 0.723\n",
      "Iteration: 19    Loss: 0.648\n",
      "Iteration: 20    Loss: 0.584\n",
      "Iteration: 21    Loss: 0.530\n",
      "Iteration: 22    Loss: 0.485\n",
      "Iteration: 23    Loss: 0.445\n",
      "Iteration: 24    Loss: 0.411\n",
      "Iteration: 25    Loss: 0.382\n",
      "Iteration: 26    Loss: 0.357\n",
      "Iteration: 27    Loss: 0.334\n",
      "Iteration: 28    Loss: 0.315\n",
      "Iteration: 29    Loss: 0.298\n",
      "Iteration: 30    Loss: 0.283\n",
      "Iteration: 31    Loss: 0.269\n",
      "Iteration: 32    Loss: 0.258\n",
      "Iteration: 33    Loss: 0.247\n",
      "Iteration: 34    Loss: 0.238\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "W = np.identity(5)\n",
    "\n",
    "learn_optimal_W(W, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try smaller alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1    Loss: 16.971\n",
      "Iteration:  2    Loss: 6.741\n",
      "Iteration:  3    Loss: 4.891\n",
      "Iteration:  4    Loss: 3.916\n",
      "Iteration:  5    Loss: 3.290\n",
      "Iteration:  6    Loss: 2.845\n",
      "Iteration:  7    Loss: 2.509\n",
      "Iteration:  8    Loss: 2.246\n",
      "Iteration:  9    Loss: 2.033\n",
      "Iteration: 10    Loss: 1.856\n",
      "Iteration: 11    Loss: 1.708\n",
      "Iteration: 12    Loss: 1.582\n",
      "Iteration: 13    Loss: 1.472\n",
      "Iteration: 14    Loss: 1.377\n",
      "Iteration: 15    Loss: 1.294\n",
      "Iteration: 16    Loss: 1.220\n",
      "Iteration: 17    Loss: 1.154\n",
      "Iteration: 18    Loss: 1.095\n",
      "Iteration: 19    Loss: 1.042\n",
      "Iteration: 20    Loss: 0.995\n",
      "Iteration: 21    Loss: 0.951\n",
      "Iteration: 22    Loss: 0.912\n",
      "Iteration: 23    Loss: 0.877\n",
      "Iteration: 24    Loss: 0.844\n",
      "Iteration: 25    Loss: 0.814\n",
      "Iteration: 26    Loss: 0.787\n",
      "Iteration: 27    Loss: 0.762\n",
      "Iteration: 28    Loss: 0.739\n",
      "Iteration: 29    Loss: 0.717\n",
      "Iteration: 30    Loss: 0.697\n",
      "Iteration: 31    Loss: 0.679\n",
      "Iteration: 32    Loss: 0.661\n",
      "Iteration: 33    Loss: 0.645\n",
      "Iteration: 34    Loss: 0.629\n",
      "Iteration: 35    Loss: 0.614\n",
      "Iteration: 36    Loss: 0.601\n",
      "Iteration: 37    Loss: 0.587\n",
      "Iteration: 38    Loss: 0.574\n",
      "Iteration: 39    Loss: 0.562\n",
      "Iteration: 40    Loss: 0.551\n",
      "Iteration: 41    Loss: 0.539\n",
      "Iteration: 42    Loss: 0.528\n",
      "Iteration: 43    Loss: 0.518\n",
      "Iteration: 44    Loss: 0.508\n",
      "Iteration: 45    Loss: 0.498\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "W = np.identity(5)\n",
    "\n",
    "learn_optimal_W(W, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then larger and smaller epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1    Loss: 47.959\n",
      "Iteration:  2    Loss: 13.114\n",
      "Iteration:  3    Loss: 8.086\n",
      "Iteration:  4    Loss: 5.951\n",
      "Iteration:  5    Loss: 4.816\n",
      "Iteration:  6    Loss: 4.078\n",
      "Iteration:  7    Loss: 3.505\n",
      "Iteration:  8    Loss: 3.028\n",
      "Iteration:  9    Loss: 2.623\n",
      "Iteration: 10    Loss: 2.278\n",
      "Iteration: 11    Loss: 1.981\n",
      "Iteration: 12    Loss: 1.727\n",
      "Iteration: 13    Loss: 1.509\n",
      "Iteration: 14    Loss: 1.323\n",
      "Iteration: 15    Loss: 1.165\n",
      "Iteration: 16    Loss: 1.031\n",
      "Iteration: 17    Loss: 0.917\n",
      "Iteration: 18    Loss: 0.820\n",
      "Iteration: 19    Loss: 0.738\n",
      "Iteration: 20    Loss: 0.667\n",
      "Iteration: 21    Loss: 0.607\n",
      "Iteration: 22    Loss: 0.556\n",
      "Iteration: 23    Loss: 0.511\n",
      "Iteration: 24    Loss: 0.473\n",
      "Iteration: 25    Loss: 0.440\n",
      "Iteration: 26    Loss: 0.410\n",
      "Iteration: 27    Loss: 0.384\n",
      "Iteration: 28    Loss: 0.361\n",
      "Iteration: 29    Loss: 0.341\n",
      "Iteration: 30    Loss: 0.322\n",
      "Iteration: 31    Loss: 0.305\n",
      "Iteration: 32    Loss: 0.290\n",
      "Iteration: 33    Loss: 0.275\n",
      "Iteration: 34    Loss: 0.262\n",
      "Iteration: 35    Loss: 0.250\n",
      "Iteration: 36    Loss: 0.239\n",
      "Iteration: 37    Loss: 0.229\n",
      "Iteration: 38    Loss: 0.219\n",
      "Iteration: 39    Loss: 0.209\n",
      "Iteration: 40    Loss: 0.201\n",
      "Iteration: 41    Loss: 0.192\n",
      "Iteration: 42    Loss: 0.184\n",
      "Iteration: 43    Loss: 0.177\n",
      "Iteration: 44    Loss: 0.170\n",
      "Iteration: 45    Loss: 0.163\n",
      "Iteration: 46    Loss: 0.156\n",
      "Iteration: 47    Loss: 0.150\n",
      "Iteration: 48    Loss: 0.144\n",
      "Iteration: 49    Loss: 0.139\n",
      "Iteration: 50    Loss: 0.133\n",
      "Iteration: 51    Loss: 0.128\n",
      "Iteration: 52    Loss: 0.123\n",
      "Iteration: 53    Loss: 0.118\n",
      "Iteration: 54    Loss: 0.114\n",
      "Iteration: 55    Loss: 0.109\n",
      "Iteration: 56    Loss: 0.105\n",
      "Iteration: 57    Loss: 0.101\n",
      "Iteration: 58    Loss: 0.097\n",
      "Iteration: 59    Loss: 0.093\n",
      "Iteration: 60    Loss: 0.090\n",
      "Iteration: 61    Loss: 0.086\n",
      "Iteration: 62    Loss: 0.083\n",
      "Iteration: 63    Loss: 0.080\n",
      "Iteration: 64    Loss: 0.077\n",
      "Iteration: 65    Loss: 0.074\n",
      "Iteration: 66    Loss: 0.071\n",
      "Iteration: 67    Loss: 0.068\n",
      "Iteration: 68    Loss: 0.066\n",
      "Iteration: 69    Loss: 0.063\n",
      "Iteration: 70    Loss: 0.061\n",
      "Iteration: 71    Loss: 0.058\n",
      "Iteration: 72    Loss: 0.056\n",
      "Iteration: 73    Loss: 0.054\n",
      "Iteration: 74    Loss: 0.052\n",
      "Iteration: 75    Loss: 0.050\n",
      "Iteration: 76    Loss: 0.048\n",
      "Iteration: 77    Loss: 0.046\n",
      "Iteration: 78    Loss: 0.044\n",
      "Iteration: 79    Loss: 0.043\n",
      "Iteration: 80    Loss: 0.041\n",
      "Iteration: 81    Loss: 0.039\n",
      "Iteration: 82    Loss: 0.038\n",
      "Iteration: 83    Loss: 0.037\n",
      "Iteration: 84    Loss: 0.035\n",
      "Iteration: 85    Loss: 0.034\n",
      "Iteration: 86    Loss: 0.033\n",
      "Iteration: 87    Loss: 0.031\n",
      "Iteration: 88    Loss: 0.030\n",
      "Iteration: 89    Loss: 0.029\n",
      "Iteration: 90    Loss: 0.028\n",
      "Iteration: 91    Loss: 0.027\n",
      "Iteration: 92    Loss: 0.026\n",
      "Iteration: 93    Loss: 0.025\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "W = np.identity(5)\n",
    "\n",
    "learn_optimal_W(W, alpha, epsilon=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unmixing sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use learned matrix $ W $ to unmix the sounds into separate data sources. Make sure you represent the resulting unmixing matrix in a way so that each row is a separate track (i.e. the matrix should have 5 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== TODO: Your code here ===============\n",
    "# Use learned matrix W to unmix the sounds into separate data sources.\n",
    "\n",
    "unmixed = W.dot(data_normalized.T)\n",
    "\n",
    "# ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving unmixed sounds. Please note that some players may not support the resulting WAV format. If that is the case, you can use Winamp to play the unmixed sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = np.max(np.abs(unmixed), axis=1)[:,  np.newaxis]\n",
    "unmixed_normalized = 0.99 * unmixed / maxs;\n",
    "\n",
    "for i in range(unmixed_normalized.shape[0]):\n",
    "    track = unmixed_normalized[i,:]\n",
    "    scipy.io.wavfile.write('unmixed/unmixed'+str(i)+'.wav', sample_rate, track)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
